# slm-llamacpp-api-server

Flask API autour de [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python) avec :

* ‚úÖ Chargement **local** de mod√®les GGUF (`/models`)
* ‚úÖ T√©l√©chargement Hugging Face Hub via `/load`
* ‚úÖ Recharge dynamique du mod√®le avec `/reload`
* ‚úÖ API **OpenAI-like** (`/v1/chat/completions`, `/v1/extract`)
* ‚úÖ **Streaming SSE** (`stream: true`) compatible OpenAI
* ‚úÖ Support `.env` pour la configuration
* ‚úÖ D√©ploiement via Docker ou en local

---

## üöÄ Installation

### 1. Cloner le d√©p√¥t

```bash
git clone https://github.com/you/slm-llamacpp-api-server.git
cd slm-api-server
```

### 2. Pr√©parer l‚Äôenvironnement

```bash
cp .env.example .env
# √âdite .env pour d√©finir le mod√®le par d√©faut (MODEL_PATH)
```

### 3. Installation locale (venv)

```bash
python -m venv .venv
source .venv/bin/activate
pip install -e .[dev]
```

---

## ‚öôÔ∏è Configuration `.env`

Exemple minimal :

```dotenv
# Dossier o√π stocker les mod√®les
MODELS_DIR=./models

# Mod√®le actif (chemin local vers un GGUF)
MODEL_PATH=./models/gemma-3-270m-it-Q4_K_M.gguf

# Param√®tres d‚Äôinf√©rence
N_THREADS=8
N_CTX=4096
N_BATCH=256
N_GPU_LAYERS=0

# R√©seau
HOST=0.0.0.0
PORT=8000
```

> üîπ **Note HF cache** :
>
> * Par d√©faut, Hugging Face utilise `~/.cache/huggingface` ‚Üí rien √† faire.
> * Si tu d√©finis `HF_HOME=./hf_cache`, cr√©e manuellement le dossier en local :
>
>   ```bash
>   mkdir hf_cache
>   ```
> * En Docker, le `Dockerfile` cr√©e d√©j√† `/hf_cache` automatiquement ‚úÖ

---

## üñ•Ô∏è D√©marrage

### Local (Flask dev server)

```bash
make dev
```

### Local (Gunicorn, prod-like)

```bash
gunicorn -w 1 -b 0.0.0.0:8000 slm_api.app:app
```

### Docker

```bash
docker compose up --build -d
```

---

## üîå Endpoints API

### `GET /health`

Ping + infos mod√®le.

### `GET /models`

Retourne le mod√®le actif + la liste des fichiers GGUF pr√©sents dans `MODELS_DIR`.

### `POST /load`

T√©l√©charge un mod√®le Hugging Face Hub dans `MODELS_DIR`.

```json
{
  "repo_id": "unsloth/gemma-3-270m-it-GGUF",
  "filename": "gemma-3-270m-it-Q4_K_M.gguf"
}
```

R√©ponse :

```json
{
  "status": "downloaded",
  "repo_id": "unsloth/gemma-3-270m-it-GGUF",
  "filename": "gemma-3-270m-it-Q4_K_M.gguf",
  "path": "/models/gemma-3-270m-it-Q4_K_M.gguf",
  "hint": "POST /reload {\"model_path\":\"/models/gemma-3-270m-it-Q4_K_M.gguf\"}"
}
```

### `POST /reload`

Recharge le mod√®le depuis un chemin local.

```json
{
  "model_path": "./models/gemma-3-270m-it-Q4_K_M.gguf",
  "n_threads": 8,
  "n_ctx": 4096
}
```

### `POST /v1/chat/completions`

Compatible OpenAI.

* `stream: false` ‚Üí r√©ponse directe
* `stream: true` ‚Üí **SSE** (Server-Sent Events)

Exemple minimal :

```json
{
  "messages": [
    {"role": "system", "content": "Tu es un assistant concis."},
    {"role": "user", "content": "Explique Gemma 3 270M en 2 phrases."}
  ],
  "stream": true,
  "temperature": 0.2
}
```

### `POST /v1/extract`

Extraction JSON avec sch√©ma optionnel.

```json
{
  "text": "We introduce NuExtract 2.0 ...",
  "schema": {
    "type": "object",
    "properties": {
      "model_name": {"type": "string"},
      "model_version": {"type": "string"},
      "model_description": {"type": "string"}
    }
  },
  "temperature": 0.0
}
```

R√©ponse :

```json
{
  "object": "extract.result",
  "created": 1699999999,
  "model": "gemma-3-270m-it-Q4_K_M.gguf",
  "latency_ms": 1234,
  "data": {
    "model_name": "NuExtract",
    "model_version": "2.0",
    "model_description": "Specialized in extracting structured info"
  }
}
```

---

## üß™ Tests

```bash
pytest -q
```

---

## üì¶ D√©ploiement Docker

Le `Dockerfile` contient d√©j√† :

```dockerfile
RUN mkdir -p /models /hf_cache
```

Donc dans le conteneur :

* `/models` est toujours pr√©sent
* `/hf_cache` est pr√™t si `HF_HOME=/hf_cache`

### Exemple Compose

```bash
docker compose up --build -d
```

Cela lance le serveur sur `http://localhost:8000`.

---

## üîê S√©curit√©

* Restreins l‚Äôacc√®s √† `/load` et `/reload` en production (authentification, firewall).
* Surveille l‚Äôespace disque (les GGUF peuvent √™tre volumineux).
* Monte un volume externe pour `/models` et `/hf_cache` pour la persistance.

---

## üìú Licence

MIT
