# slm-api-server

---
**Vibe-cod√© avec GPT-5**

---

Flask API autour de [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python) avec :

* ‚úÖ Chargement **local** de mod√®les GGUF
* ‚úÖ T√©l√©chargement Hugging Face Hub via `/load`
* ‚úÖ **Auto-load au d√©marrage** (si le fichier manque)
* ‚úÖ Recharge dynamique du mod√®le via `/reload`
* ‚úÖ API **OpenAI-like** (`/v1/chat/completions`, `/v1/extract`)
* ‚úÖ **Streaming SSE** (`stream: true`) compatible OpenAI
* ‚úÖ Support `.env` (via `python-dotenv`)
* ‚úÖ D√©ploiement **Docker** ou local

---

## üöÄ Installation

### Cloner & pr√©parer

```bash
git clone https://github.com/you/slm-api-server.git
cd slm-api-server
cp .env.example .env
# √©dite .env selon le contexte (voir section Configuration)
```

### Installation locale (venv)

```bash
python -m venv .venv
source .venv/bin/activate
pip install -e .[dev]
```

---

## ‚öôÔ∏è Configuration (.env)

### Variables principales

| Variable                                        | R√¥le                                                                   | Exemple / Valeur recommand√©e                                  |
| ----------------------------------------------- | ---------------------------------------------------------------------- | ------------------------------------------------------------- |
| `MODELS_DIR`                                    | Dossier **dans l‚Äôenvironnement o√π tourne l‚Äôapp** pour stocker les GGUF | `/models` (en Docker) ou `./models` (local)                   |
| `MODEL_PATH`                                    | **Chemin du fichier GGUF actif** (le mod√®le que l‚ÄôAPI utilisera)       | `/models/gemma-3-270m-it-Q4_K_M.gguf` (peut √™tre laiss√© vide) |
| `AUTO_REPO_ID`                                  | Repo HF pour auto-load au boot si le fichier manque                    | `unsloth/gemma-3-270m-it-GGUF`                                |
| `AUTO_FILENAME`                                 | Nom du fichier `.gguf` √† r√©cup√©rer sur HF                              | `gemma-3-270m-it-Q4_K_M.gguf`                                 |
| `AUTO_IF_MISSING`                               | `true/false` ‚Äî ne t√©l√©charge **que si** le fichier local est absent    | `true`                                                        |
| `AUTO_PRELOAD`                                  | `true/false` ‚Äî charge le mod√®le **en m√©moire au boot**                 | `false` (lazy)                                                |
| `N_THREADS`, `N_CTX`, `N_BATCH`, `N_GPU_LAYERS` | Param√®tres d‚Äôinf√©rence CPU (llama.cpp)                                 | `8`, `4096`, `256`, `0`                                       |
| `HOST`, `PORT`                                  | R√©seau API                                                             | `0.0.0.0`, `8000`                                             |
| `HF_HOME` (optionnel)                           | Dossier cache HF                                                       | `/hf_cache` (Docker)                                          |

> ‚ÑπÔ∏è **`MODEL_PATH` = ‚Äúpointeur‚Äù vers le fichier GGUF actif.**
>
> * Si le fichier existe : l‚ÄôAPI peut l‚Äôutiliser imm√©diatement (lazy ou pr√©-charg√© selon `AUTO_PRELOAD`).
> * S‚Äôil n‚Äôexiste pas et que `AUTO_REPO_ID`/`AUTO_FILENAME` sont fournis, l‚Äôapp **t√©l√©charge** ce mod√®le dans `MODELS_DIR` au d√©marrage (si `AUTO_IF_MISSING=true`) et ajuste `MODEL_PATH`.
> * Sinon, laisser `MODEL_PATH` vide et piloter tout via `/load` puis `/reload`.

### Exemple de `.env` (Docker recommand√©)

```dotenv
# Stockage des mod√®les dans le conteneur
MODELS_DIR=/models
MODEL_PATH=/models/gemma-3-270m-it-Q4_K_M.gguf

# Auto-load au d√©marrage si le fichier manque
AUTO_REPO_ID=unsloth/gemma-3-270m-it-GGUF
AUTO_FILENAME=gemma-3-270m-it-Q4_K_M.gguf
AUTO_IF_MISSING=true
AUTO_PRELOAD=false

# Inf√©rence CPU
N_THREADS=8
N_CTX=4096
N_BATCH=256
N_GPU_LAYERS=0

# R√©seau
HOST=0.0.0.0
PORT=8000

# (Optionnel) Cache HF
# HF_HOME=/hf_cache
```

---

## üñ•Ô∏è D√©marrage

### Local (Flask dev server)

```bash
make dev
```

### Local (gunicorn, prod-like)

```bash
gunicorn -w 1 -b 0.0.0.0:8000 slm_api.app:app
```

### Docker

* **Sans persistance host** (simple) ‚Äî *les mod√®les sont stock√©s dans le conteneur* :

```yaml
# docker-compose.yml (extrait)
services:
  api:
    build: .
    env_file: .env
    ports:
      - "${PORT:-8000}:8000"
    restart: unless-stopped
```

* **Avec persistance** (recommand√© en prod) :

```yaml
services:
  api:
    build: .
    env_file: .env
    ports:
      - "${PORT:-8000}:8000"
    volumes:
      - ./models:/models
      - ./hf_cache:/hf_cache
    restart: unless-stopped
```

> üí° Si les volumes sont comment√©s, les `.gguf` seront **dans le conteneur** (perdus √† la suppression).
> Avec volumes, ils sont persistants c√¥t√© host.

---

## ü©∫ Sant√© & cycle de vie du mod√®le

### `/health` ‚Äî statuts

* `unloaded` : `MODEL_PATH` non d√©fini ou fichier absent.
  ‚Üí utilise `/load` puis `/reload`, ou configure `AUTO_REPO_ID`/`AUTO_FILENAME`.
* `available` : le fichier `MODEL_PATH` **existe**, mais le mod√®le n‚Äôa pas encore √©t√© charg√© en m√©moire.
  ‚Üí un appel `/reload` ou une premi√®re requ√™te de compl√©tion le chargera.
* `ready` : le mod√®le est **charg√© en m√©moire**, l‚ÄôAPI est fully ready.

Exemple :

```bash
curl -s http://localhost:8000/health | jq .
```

### `/models` ‚Äî vue d‚Äôensemble

Renvoie l‚Äô√©tat actif + la liste des `.gguf` trouv√©s :

```json
{
  "object": "list",
  "active": {
    "path": "/models/gemma-3-270m-it-Q4_K_M.gguf",
    "basename": "gemma-3-270m-it-Q4_K_M.gguf",
    "exists": true,
    "loaded": false,
    "state": "available",
    "config": { "model_path": "gemma-3-270m-it-Q4_K_M.gguf", "n_ctx": 4096, ... }
  },
  "available": [
    { "name": "gemma-3-270m-it-Q4_K_M.gguf", "path": "/models/gemma-3-270m-it-Q4_K_M.gguf", "size_bytes": 1234 }
  ],
  "hint": "Switch with POST /reload {\"model_path\": \"/models/<file>.gguf\"}",
  "boot_info": { "...": "..." }
}
```

---

## üîå Endpoints API

### `POST /load`

T√©l√©charge un mod√®le depuis Hugging Face Hub dans `MODELS_DIR` (ne fait **pas** de reload auto).

```bash
curl -sX POST http://localhost:8000/load \
  -H "Content-Type: application/json" \
  -d '{"repo_id":"unsloth/gemma-3-270m-it-GGUF","filename":"gemma-3-270m-it-Q4_K_M.gguf"}' | jq .
```

R√©ponse (ex.) :

```json
{
  "status": "downloaded",
  "repo_id": "unsloth/gemma-3-270m-it-GGUF",
  "filename": "gemma-3-270m-it-Q4_K_M.gguf",
  "path": "/models/gemma-3-270m-it-Q4_K_M.gguf",
  "hint": "POST /reload {\"model_path\": \"/models/gemma-3-270m-it-Q4_K_M.gguf\"}"
}
```

### `POST /reload`

Active un mod√®le local et (re)charge en m√©moire.
Params utiles : `model_path`, `n_ctx`, `n_threads`, `n_batch`, `n_gpu_layers`, `verbose`.

```bash
curl -sX POST http://localhost:8000/reload \
  -H "Content-Type: application/json" \
  -d '{"model_path":"/models/gemma-3-270m-it-Q4_K_M.gguf","n_threads":8,"n_ctx":4096}' | jq .
```

### `POST /v1/chat/completions`

OpenAI-like.

* R√©ponse directe si `stream: false` (par d√©faut)
* **SSE** si `stream: true` (chunking type OpenAI + terminaison `data: [DONE]`)

```bash
curl -N -s http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "system", "content": "Tu es un assistant concis."},
      {"role": "user", "content": "Explique Gemma 3 270M en 2 phrases."}
    ],
    "stream": true,
    "temperature": 0.2,
    "top_p": 0.95,
    "top_k": 64
  }'
```

### `POST /v1/extract`

Extraction JSON avec sch√©ma optionnel (`response_format` transmis si support√©).

```bash
curl -s http://localhost:8000/v1/extract \
  -H "Content-Type: application/json" \
  -d '{
    "text": "We introduce NuExtract 2.0 ...",
    "schema": {
      "type": "object",
      "properties": {
        "model_name": {"type": "string"},
        "model_version": {"type": "string"},
        "model_description": {"type": "string"}
      }
    },
    "temperature": 0.0
  }' | jq .
```

---

## üß† Auto-load : ce qui se passe au d√©marrage

1. L‚Äôapp lit `.env`.
2. Si `MODEL_PATH` **pointe vers un fichier existant**, elle l‚Äôutilisera (lazy ou `AUTO_PRELOAD`).
3. Sinon, si **`AUTO_REPO_ID` + `AUTO_FILENAME`** sont fournis et `AUTO_IF_MISSING=true` :

   * l‚Äôapp **t√©l√©charge** le `.gguf` dans `MODELS_DIR`,
   * met √† jour `MODEL_PATH` vers ce fichier,
   * si `AUTO_PRELOAD=true`, **charge** le mod√®le en m√©moire imm√©diatement.
4. √Ä d√©faut, `/health` reste en `unloaded` avec des **hints** d‚Äôaction.

---

## üß™ Tests

```bash
pytest -q
```

---

## üîê S√©curit√©

* Prot√©gez `/load` et `/reload` en prod (authN, firewall, r√©seau priv√©).
* Surveillez l‚Äôespace disque (les GGUF sont volumineux).
* En Docker, pr√©f√©rer des **volumes** pour persister `/models` et `/hf_cache`.

---

## ‚ùìD√©pannage rapide

* `/health` ‚Üí `unloaded` : le fichier `MODEL_PATH` est manquant.
  ‚Üí utilisez `/load` puis `/reload`, ou `AUTO_REPO_ID`/`AUTO_FILENAME`.
* Vous ne voyez pas `/models` en Docker :
  ‚Üí **dans le conteneur**, le chemin doit √™tre absolu (`/models/...`).
  ‚Üí si vous avez comment√© les volumes, tout reste **dans le conteneur**.
* Valider l‚Äô√©tat dans le conteneur :

  ```bash
  docker compose exec api sh -lc 'echo MODELS_DIR=$MODELS_DIR; echo MODEL_PATH=$MODEL_PATH; ls -la /models'
  ```

---

## üìú Licence

MIT